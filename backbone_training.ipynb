{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_-SDeDsv_U"
      },
      "source": [
        "# INSTALL DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUQb3CoZVxDC",
        "outputId": "42f40e5f-73bb-4a39-f296-940f0b89ecf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.14.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.3)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.4)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1XrJAoRuY3U8",
        "outputId": "6643352c-4cc8-4b2c-97ef-e41b4940f812"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import torch_xla\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.amp.syncfree.adamw as adamw'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc, confusion_matrix, jaccard_score\n",
        "import gc\n",
        "#from torch.cuda.amp import autocast, GradScaler\n",
        "from torchinfo import summary\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.amp.syncfree.adamw as adamw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIBlrhAyhoj1"
      },
      "source": [
        "# DATALOADER\n",
        "\n",
        "This block defines the Dataloader used for the backbone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHHcQRtblWpM"
      },
      "outputs": [],
      "source": [
        "def creator_path_slices(patient_dir):\n",
        "\n",
        "    return [os.path.join(patient_dir,s) for s in os.listdir(patient_dir)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL4DxRmY9uWe"
      },
      "outputs": [],
      "source": [
        "def get_augmentations():\n",
        "  rotation = random.randint(-10,10)\n",
        "  return A.Compose([A.Affine(rotate=(rotation,rotation),p=1),\n",
        "                    A.Affine(\n",
        "                        scale=(1 - 0.15, 1 + 0.15),\n",
        "                        translate_percent=(-0.1, 0.1),\n",
        "                        p=0.8, border_mode=cv2.BORDER_CONSTANT),\n",
        "                    A.RandomBrightnessContrast(\n",
        "                    brightness_limit=0.1,\n",
        "                    contrast_limit=0.1,\n",
        "                    p=0.5\n",
        "                ),\n",
        "                A.GaussianBlur(blur_limit=(3, 5), p=0.1),\n",
        "                A.GaussNoise(std_range=(0.05, 0.1), p=0.2),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bik1YhW9RMJw"
      },
      "outputs": [],
      "source": [
        "class TomosynthesisDataset(Dataset):\n",
        "    def __init__(self, csv_path, images_folder, augmentations=None):\n",
        "        self.metadata = pd.read_csv(csv_path)\n",
        "        self.images_folder = images_folder\n",
        "        self.augmentations = augmentations\n",
        "        self.data = []\n",
        "\n",
        "        for i in range(len(self.metadata)):\n",
        "            patient_id = self.metadata.iloc[i]['PatientID']\n",
        "            target_slice = self.metadata.iloc[i]['Slice_representativo']\n",
        "            intermediate_folder = self.metadata.iloc[i]['Intermedia']\n",
        "            classification = self.metadata.iloc[i][\"Clasificacion\"]\n",
        "            patient_folder = os.path.join(images_folder, patient_id)\n",
        "\n",
        "            entry = {\n",
        "                'patient_id': patient_id,\n",
        "                'target_slice': target_slice,\n",
        "                'intermediate_folder': os.path.join(patient_folder, intermediate_folder),\n",
        "                'classification': classification\n",
        "            }\n",
        "\n",
        "            if classification == 1:\n",
        "\n",
        "              self.data.append((entry, False))  # False --> original image\n",
        "\n",
        "              self.data.append((entry, True))   # True --> augmented image\n",
        "            else:\n",
        "              self.data.append((entry,False))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry, apply_augmentation = self.data[idx]\n",
        "\n",
        "        distancia = 100\n",
        "        target_slice_subsample_position = 0\n",
        "        intermediate_folder = entry['intermediate_folder']\n",
        "        target_slice = entry['target_slice']\n",
        "\n",
        "        paths_slices = creator_path_slices(intermediate_folder)\n",
        "        if len(paths_slices) != 27:\n",
        "            raise ValueError(f\"Expected 27 slices, got {len(paths_slices)} for patient {entry['patient_id']}\")\n",
        "\n",
        "        for i in range(len(paths_slices)):\n",
        "            name_slice = paths_slices[i].split('/')[-1]\n",
        "            num_slice = int(name_slice.split('.')[0])\n",
        "\n",
        "            if abs(target_slice - num_slice) < distancia:\n",
        "                target_slice_subsample_position = i\n",
        "                distancia = abs(target_slice - num_slice)\n",
        "\n",
        "        slices = []\n",
        "        transform = transforms.ToTensor()\n",
        "        for path in paths_slices:\n",
        "            if not os.path.exists(path):\n",
        "                raise FileNotFoundError(f\"Slice not found: {path}\")\n",
        "\n",
        "            slice_img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "            slice_img = cv2.cvtColor(slice_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if apply_augmentation and self.augmentations is not None:\n",
        "                slice_img = self.augmentations(image=slice_img)['image']\n",
        "\n",
        "            slice_tensor = transform(slice_img)\n",
        "            slices.append(slice_tensor)\n",
        "\n",
        "        slices_tensor = torch.stack(slices)\n",
        "        return slices_tensor, torch.tensor(target_slice_subsample_position, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nURqTUUPHAZ8"
      },
      "source": [
        "## Prueba dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OtNvdzv0Q0e",
        "outputId": "0b72e7ad-657e-4cfe-e56f-d19f4577e24a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"train_dataset = TomosynthesisDataset(\\n      csv_path='/content/drive/MyDrive/slices.csv',\\n      images_folder='/content/drive/MyDrive/PruebaColab',\\n      augmentations = get_augmentations()\\n    )\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"train_dataset = TomosynthesisDataset(\n",
        "      csv_path='/content/drive/MyDrive/slices.csv',\n",
        "      images_folder='/content/drive/MyDrive/PruebaColab',\n",
        "      augmentations = get_augmentations()\n",
        "    )\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS5H8NfPHcTz",
        "outputId": "e0e70984-876f-4329-d3d0-6e95fda0f923"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Crear el DataLoader\\ndata_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\\n\\n# Imprimir algunos labels para verificar\\nfor i, (slices, labels) in enumerate(data_loader):\\n    print(f\"Lote {i + 1} - Labels: {labels.size(0)}\")\\n\\n    if i == 5:  # Mostrar solo los primeros 5 lotes\\n        break'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"# Crear el DataLoader\n",
        "data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Imprimir algunos labels para verificar\n",
        "for i, (slices, labels) in enumerate(data_loader):\n",
        "    print(f\"Lote {i + 1} - Labels: {labels.size(0)}\")\n",
        "\n",
        "    if i == 5:  # Mostrar solo los primeros 5 lotes\n",
        "        break\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XshGvANqn_Aq",
        "outputId": "c2af7474-50a3-48eb-f7fb-0b56806da148"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import matplotlib.pyplot as plt\\nimport numpy as np\\ndataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\\n\\n# Obtener un batch del dataloader\\ndata_iter = iter(dataloader)\\nslices_tensor, labels = next(data_iter)  # slices_tensor tiene forma [batch, 27, C, H, W]\\n\\n# Seleccionar un ejemplo del batch (por ejemplo, el primer paciente)\\nindex = 0  # Puedes cambiar esto para ver otros casos\\nsample_slices = slices_tensor[index]  # [27, C, H, W]\\n\\n# Visualizar algunas slices del volumen\\nfig, axes = plt.subplots(1, 5, figsize=(15, 5))  # Mostrar 5 slices\\nfor i, ax in enumerate(axes):\\n    slice_img = sample_slices[i].permute(1, 2, 0).numpy()  # Convertir tensor a imagen\\n    slice_img = (slice_img * 255).astype(np.uint8)  # Reescalar si es necesario\\n    ax.imshow(slice_img)\\n    ax.set_title(f\"Slice {i}\")\\n    ax.axis(\"off\")\\n\\nplt.show()\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Obtener un batch del dataloader\n",
        "data_iter = iter(dataloader)\n",
        "slices_tensor, labels = next(data_iter)  # slices_tensor tiene forma [batch, 27, C, H, W]\n",
        "\n",
        "# Seleccionar un ejemplo del batch (por ejemplo, el primer paciente)\n",
        "index = 0  # Puedes cambiar esto para ver otros casos\n",
        "sample_slices = slices_tensor[index]  # [27, C, H, W]\n",
        "\n",
        "# Visualizar algunas slices del volumen\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 5))  # Mostrar 5 slices\n",
        "for i, ax in enumerate(axes):\n",
        "    slice_img = sample_slices[i].permute(1, 2, 0).numpy()  # Convertir tensor a imagen\n",
        "    slice_img = (slice_img * 255).astype(np.uint8)  # Reescalar si es necesario\n",
        "    ax.imshow(slice_img)\n",
        "    ax.set_title(f\"Slice {i}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3RCI_7JOxpP",
        "outputId": "92fddc4d-e455-4f5e-add3-d741927cd84e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for batch_slices, batch_targets in dataloader:\\n    # batch_slices: [batch_size, num_slices, channels, height, width]\\n    # batch_targets: [batch_size]\\n    print(batch_slices.shape)\\n    print(batch_targets)\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"for batch_slices, batch_targets in dataloader:\n",
        "    # batch_slices: [batch_size, num_slices, channels, height, width]\n",
        "    # batch_targets: [batch_size]\n",
        "    print(batch_slices.shape)\n",
        "    print(batch_targets)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYLTX4xbmOm"
      },
      "source": [
        "# ARCHITECTURE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Module\n",
        "\n",
        "This module is used to take the most representative slice in a tomosynthesis stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5HVHxg3DOf5"
      },
      "outputs": [],
      "source": [
        "class RepresentativeSliceDetector(nn.Module):\n",
        "    def __init__(self, feature_dim=512, hidden_dim=256, dropout=0.1):\n",
        "        super(RepresentativeSliceDetector, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(self.feature_dim)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        \"\"\"\n",
        "        Returns the attention logits and unweighted feature maps.\n",
        "\n",
        "        Args:\n",
        "            feature_map: Tensor of size (batch_size, num_slices, feature_dim, H, W)\n",
        "\n",
        "        Returns:\n",
        "            feature_map: Tensor of size (batch_size, num_slices, feature_dim, H, W)\n",
        "            logits: Tensor of size (batch_size, num_slices, 1) (logits for each slice)\n",
        "        \"\"\"\n",
        "\n",
        "        features = feature_map.mean(dim=(3, 4))  # (batch_size, num_slices, feature_dim)\n",
        "\n",
        "        features = self.layer_norm(features)\n",
        "        logits = self.attention(features)\n",
        "\n",
        "        return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extractor\n",
        "\n",
        "Returns the feature map of each slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEEcL7-EZBSv"
      },
      "outputs": [],
      "source": [
        "class ResNet50FeatureExtractor(nn.Module):\n",
        "    def __init__(self, weights=ResNet50_Weights.DEFAULT, dropout=0.1):\n",
        "        super(ResNet50FeatureExtractor, self).__init__()\n",
        "        self.resnet50 = models.resnet50(weights=weights)\n",
        "        self.resnet50.to(torch.bfloat16)\n",
        "        self.stem = nn.Sequential(\n",
        "            self.resnet50.conv1,\n",
        "            self.resnet50.bn1,\n",
        "            self.resnet50.relu,\n",
        "            self.resnet50.maxpool  # 1/4 resolution\n",
        "        )\n",
        "\n",
        "        self.layer1 = self.resnet50.layer1  # C2 (this layer is not going to be used in the FPN)\n",
        "        self.layer2 = self.resnet50.layer2  # C3\n",
        "        self.layer3 = self.resnet50.layer3  # C4\n",
        "        self.layer4 = self.resnet50.layer4  # C5\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_slices, C, H, W = x.size()\n",
        "        x = x.view(batch_size * num_slices, C, H, W)\n",
        "\n",
        "        x = self.stem(x)\n",
        "        c2 = self.layer1(x)  # (B*N, 256, H/4, W/4)\n",
        "        c3 = self.layer2(c2)  # (B*N, 512, H/8, W/8)\n",
        "        c4 = self.layer3(c3)  # (B*N, 1024, H/16, W/16)\n",
        "        c5 = self.layer4(c4)  # (B*N, 2048, H/32, W/32)\n",
        "\n",
        "        def reshape(f): return f.view(batch_size, num_slices, f.size(1), f.size(2), f.size(3))\n",
        "\n",
        "        return {\n",
        "            'c3': reshape(c3),\n",
        "            'c4': reshape(c4),\n",
        "            'c5': reshape(c5)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbVzSmrpskSR"
      },
      "source": [
        "## MRI with FPN\n",
        "\n",
        "The attention module is feed with characteristics of the c5 layer after been passed thought a FPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvZBAg_GGP13"
      },
      "outputs": [],
      "source": [
        "class ModifiedResNet50Backbone(nn.Module):\n",
        "    def __init__(self, out_channels=512):\n",
        "        super(ModifiedResNet50Backbone, self).__init__()\n",
        "\n",
        "        try:\n",
        "          self.device = xm.xla_device()\n",
        "\n",
        "        except:\n",
        "          self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.feature_extractor = ResNet50FeatureExtractor()\n",
        "        self.attention_module = RepresentativeSliceDetector(feature_dim=self.out_channels)\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[512, 1024, 2048],\n",
        "            out_channels=self.out_channels\n",
        "        ).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        top_indices_logits = None\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        c3, c4, c5 = features[\"c3\"], features[\"c4\"], features[\"c5\"]\n",
        "\n",
        "        B, N, C3, H3, W3 = c3.shape\n",
        "        _, _, C4, H4, W4 = c4.shape\n",
        "        _, _, C5, H5, W5 = c5.shape\n",
        "\n",
        "        c3_fpn = c3.view(B * N, C3, H3, W3)\n",
        "        c4_fpn = c4.view(B * N, C4, H4, W4)\n",
        "        c5_fpn = c5.view(B * N, C5, H5, W5)\n",
        "\n",
        "\n",
        "        fpn_out = self.fpn(OrderedDict({\n",
        "            \"0\": c3_fpn,\n",
        "            \"1\": c4_fpn,\n",
        "            \"2\": c5_fpn\n",
        "        }))\n",
        "        features_fpn = {k: v.view(B, N, self.out_channels, v.shape[2], v.shape[3]) for k, v in fpn_out.items()} #recuperar shape original\n",
        "\n",
        "        top_indices_logits = self.attention_module(features_fpn['2'])\n",
        "\n",
        "        return top_indices_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhwymGh-so42"
      },
      "source": [
        "## MRI without FPN\n",
        "\n",
        "The attention module is feed with characteristics of the c5 layer directly from the feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptUVXa6fwoIZ"
      },
      "outputs": [],
      "source": [
        "class ModifiedResNet50Backbone(nn.Module): \n",
        "    def __init__(self, out_channels=2048):\n",
        "        super(ModifiedResNet50Backbone, self).__init__()\n",
        "\n",
        "        try:\n",
        "          self.device = xm.xla_device()\n",
        "        except:\n",
        "          self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.feature_extractor = ResNet50FeatureExtractor()\n",
        "        self.attention_module = RepresentativeSliceDetector(feature_dim=self.out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        top_indices_logits = None\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        c3, c4, c5 = features[\"c3\"], features[\"c4\"], features[\"c5\"]\n",
        "\n",
        "        top_indices_logits = self.attention_module(c5)\n",
        "\n",
        "        return top_indices_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc6wdxFDDtfu"
      },
      "source": [
        "## Data flow of the backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0pXUs5tSmBH",
        "outputId": "a07fe5da-3611-43b2-bad1-6226b6ffeba4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "ModifiedResNet50Backbone                      [8, 27]                   --\n",
              "â”œâ”€ResNet50FeatureExtractor: 1-1               [8, 27, 2048, 8, 8]       2,049,000\n",
              "â”‚    â””â”€Sequential: 2-1                        [216, 64, 64, 64]         --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-1                       [216, 64, 128, 128]       9,408\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-2                  [216, 64, 128, 128]       128\n",
              "â”‚    â”‚    â””â”€ReLU: 3-3                         [216, 64, 128, 128]       --\n",
              "â”‚    â”‚    â””â”€MaxPool2d: 3-4                    [216, 64, 64, 64]         --\n",
              "â”‚    â””â”€Sequential: 2-2                        [216, 256, 64, 64]        --\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-5                   [216, 256, 64, 64]        75,008\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-6                   [216, 256, 64, 64]        70,400\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-7                   [216, 256, 64, 64]        70,400\n",
              "â”‚    â””â”€Sequential: 2-3                        [216, 512, 32, 32]        --\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-8                   [216, 512, 32, 32]        379,392\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-9                   [216, 512, 32, 32]        280,064\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-10                  [216, 512, 32, 32]        280,064\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-11                  [216, 512, 32, 32]        280,064\n",
              "â”‚    â””â”€Sequential: 2-4                        [216, 1024, 16, 16]       --\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-12                  [216, 1024, 16, 16]       1,512,448\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-13                  [216, 1024, 16, 16]       1,117,184\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-14                  [216, 1024, 16, 16]       1,117,184\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-15                  [216, 1024, 16, 16]       1,117,184\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-16                  [216, 1024, 16, 16]       1,117,184\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-17                  [216, 1024, 16, 16]       1,117,184\n",
              "â”‚    â””â”€Sequential: 2-5                        [216, 2048, 8, 8]         --\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-18                  [216, 2048, 8, 8]         6,039,552\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-19                  [216, 2048, 8, 8]         4,462,592\n",
              "â”‚    â”‚    â””â”€Bottleneck: 3-20                  [216, 2048, 8, 8]         4,462,592\n",
              "â”œâ”€RepresentativeSliceDetector: 1-2            [8, 27]                   --\n",
              "â”‚    â””â”€LayerNorm: 2-6                         [8, 27, 2048]             4,096\n",
              "â”‚    â””â”€Sequential: 2-7                        [8, 27, 1]                --\n",
              "â”‚    â”‚    â””â”€Linear: 3-21                      [8, 27, 256]              524,544\n",
              "â”‚    â”‚    â””â”€ReLU: 3-22                        [8, 27, 256]              --\n",
              "â”‚    â”‚    â””â”€Dropout: 3-23                     [8, 27, 256]              --\n",
              "â”‚    â”‚    â””â”€Linear: 3-24                      [8, 27, 1]                257\n",
              "===============================================================================================\n",
              "Total params: 26,085,929\n",
              "Trainable params: 26,085,929\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.TERABYTES): 1.15\n",
              "===============================================================================================\n",
              "Input size (MB): 169.87\n",
              "Forward/backward pass size (MB): 50172.05\n",
              "Params size (MB): 96.15\n",
              "Estimated Total Size (MB): 50438.07\n",
              "==============================================================================================="
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = ModifiedResNet50Backbone()\n",
        "summary(modelo, input_size=(8,27, 3, 256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KTv2rUatlF8"
      },
      "source": [
        "# LOAD DATALOADERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le4TqXXTsuBg"
      },
      "source": [
        "## Load data from 1 big dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxjA6oyB2i-f",
        "outputId": "93c90c6d-fee4-4d9c-dff1-b37be1860080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TamaÃ±o train batches: 289.\n",
            "TamaÃ±o val batches: 73.\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('csv_path')\n",
        "labels = df['Clasificacion'].tolist()\n",
        "indices = list(range(len(df)))\n",
        "\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "base_dataset = TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='image_folder_path',\n",
        "    augmentations=None  \n",
        ")\n",
        "\n",
        "train_dataset = Subset(TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='/content/drive/MyDrive/test+valid_png',\n",
        "    augmentations=get_augmentations()\n",
        "), train_idx)\n",
        "\n",
        "val_dataset = Subset(TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='image_folder_path',\n",
        "    augmentations=None\n",
        "), val_idx)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "\n",
        "  )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "print(f'TamaÃ±o train batches: {len(train_dataloader)}.')\n",
        "print(f'TamaÃ±o val batches: {len(val_dataloader)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2qy8Q_-xyNr"
      },
      "source": [
        "## Load data from a split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeKLBHUsxxJK",
        "outputId": "e3259891-5509-4fd7-fa5f-86a263b84149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TamaÃ±o train batches: 101.\n",
            "TamaÃ±o val batches: 20.\n"
          ]
        }
      ],
      "source": [
        "def get_augmentations():\n",
        "  rotation = random.randint(-10,10)\n",
        "  return A.Compose([A.Affine(rotate=(rotation,rotation),p=1)\n",
        "    ])\n",
        "\n",
        "train_dataset = TomosynthesisDataset(\n",
        "      csv_path= \"csv_train_path\",\n",
        "      images_folder='image_folder_train_path',\n",
        "      augmentations = get_augmentations()\n",
        "    )\n",
        "\n",
        "val_dataset = TomosynthesisDataset(\n",
        "  csv_path='csv_test_path',\n",
        "  images_folder='image_folder_val_path',\n",
        "  )\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=2\n",
        "  )\n",
        "\n",
        "\n",
        "print(f'TamaÃ±o train batches: {len(train_dataloader)}.')\n",
        "print(f'TamaÃ±o val batches: {len(val_dataloader)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAlv_vaCtrKk"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgPGez3RhM3Y"
      },
      "source": [
        "## GPU training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBay3K0z1csc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=20, lr=1e-5, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=7)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    training_history = {\n",
        "        \"train_loss\": [], \"train_acc\": [], \"train_precision\": [], \"train_recall\": [], \"train_iou\": [],\n",
        "        \"val_loss\": [], \"val_acc\": [], \"val_precision\": [], \"val_recall\": [], \"val_iou\": [],\n",
        "        \"val_probs\": [], \"val_labels\": []\n",
        "    }\n",
        "\n",
        "    patience = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ### ðŸ”¹ Training ðŸ”¹ ###\n",
        "        model.train()\n",
        "        train_loss, train_correct, total_samples = 0.0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        train_loader_tqdm = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False)\n",
        "\n",
        "        for images, labels in train_loader_tqdm:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                feature_maps, logits = model(images)\n",
        "                loss = loss_fn(logits, labels.long())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= total_samples\n",
        "        train_acc = train_correct / total_samples\n",
        "        train_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        train_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        train_iou = jaccard_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        training_history[\"train_loss\"].append(train_loss)\n",
        "        training_history[\"train_acc\"].append(train_acc)\n",
        "        training_history[\"train_precision\"].append(train_precision)\n",
        "        training_history[\"train_recall\"].append(train_recall)\n",
        "        training_history[\"train_iou\"].append(train_iou)\n",
        "\n",
        "        ### ðŸ”¹ Validation ðŸ”¹ ###\n",
        "        model.eval()\n",
        "        val_loss, val_correct, total_val_samples = 0.0, 0, 0\n",
        "        all_val_preds, all_val_labels, all_val_probs = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loader_tqdm = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False)\n",
        "\n",
        "            for images, labels in val_loader_tqdm:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                with autocast():\n",
        "                    feature_maps, logits = model(images)\n",
        "                    loss = loss_fn(logits, labels.long())\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                total_val_samples += labels.size(0)\n",
        "\n",
        "                all_val_preds.extend(preds.cpu().numpy())\n",
        "                all_val_labels.extend(labels.cpu().numpy())\n",
        "                all_val_probs.extend(torch.softmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= total_val_samples\n",
        "        val_acc = val_correct / total_val_samples\n",
        "        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "        val_iou = jaccard_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        training_history[\"val_loss\"].append(val_loss)\n",
        "        training_history[\"val_acc\"].append(val_acc)\n",
        "        training_history[\"val_precision\"].append(val_precision)\n",
        "        training_history[\"val_recall\"].append(val_recall)\n",
        "        training_history[\"val_iou\"].append(val_iou)\n",
        "        training_history[\"val_probs\"].append(np.array(all_val_probs))\n",
        "        training_history[\"val_labels\"].append(np.array(all_val_labels))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train Prec: {train_precision:.4f}, Train Rec: {train_recall:.4f}, Train IoU: {train_iou:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Prec: {val_precision:.4f}, Val Rec: {val_recall:.4f}, Val IoU: {val_iou:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(best_model_state, \"best_model.pth\")\n",
        "            print(f\"Best model in epoch {epoch}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping activado.\")\n",
        "            break\n",
        "\n",
        "        with open(\"training_history.pkl\", \"wb\") as f:\n",
        "            pickle.dump(training_history, f)\n",
        "\n",
        "    return training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuXbbdMlhRQe"
      },
      "source": [
        "## ENTRENAMIENTO TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7w_C9ihwSE4"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=90):\n",
        "    device = xm.xla_device()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = adamw.AdamW([\n",
        "                {'params': model.feature_extractor.parameters(), 'lr': 1e-5},\n",
        "                {'params': model.attention_module.parameters(), 'lr': 1e-4}\n",
        "            ], weight_decay=1e-5)\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=8)\n",
        "\n",
        "    best_val_loss = 100\n",
        "    training_history = {\n",
        "        \"train_loss\": [], \"train_acc\": [], \"train_precision\": [], \"train_recall\": [], \"train_iou\": [],\n",
        "        \"val_loss\": [], \"val_acc\": [], \"val_precision\": [], \"val_recall\": [], \"val_iou\": [],\n",
        "        \"val_probs\": [], \"val_labels\": [], \"train_preds\": [], \"train_labels\": [], \"val_preds\": []\n",
        "    }\n",
        "\n",
        "    patience = 17\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      #-----------------------------------TRAIN------------------------------------------\n",
        "      model.train()\n",
        "      train_loss, train_correct, total_samples = 0.0, 0, 0\n",
        "      all_train_preds, all_train_labels = [], []\n",
        "\n",
        "      train_loader_tqdm = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=True)\n",
        "\n",
        "      for images, labels in train_loader_tqdm:\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "              logits = model(images)\n",
        "              loss = loss_fn(logits, labels.long())\n",
        "\n",
        "          loss.backward()\n",
        "          xm.optimizer_step(optimizer)\n",
        "\n",
        "          train_loss += loss.item() * images.size(0)\n",
        "          preds = torch.argmax(logits, dim=1)\n",
        "          train_correct += ((abs(preds - labels) <= 2)).sum().item()\n",
        "          total_samples += labels.size(0)\n",
        "\n",
        "          all_train_preds.extend(preds.cpu().numpy())\n",
        "          all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "          train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "          del logits, loss, images, labels, preds\n",
        "          xm.mark_step()\n",
        "\n",
        "      train_loss /= total_samples\n",
        "      train_acc = train_correct / total_samples\n",
        "      train_precision = precision_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n",
        "      train_recall = recall_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n",
        "      train_iou = jaccard_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n",
        "\n",
        "      training_history[\"train_loss\"].append(train_loss)\n",
        "      training_history[\"train_acc\"].append(train_acc)\n",
        "      training_history[\"train_precision\"].append(train_precision)\n",
        "      training_history[\"train_recall\"].append(train_recall)\n",
        "      training_history[\"train_iou\"].append(train_iou)\n",
        "      training_history[\"train_preds\"].append(np.array(all_train_preds))\n",
        "      training_history[\"train_labels\"].append(np.array(all_train_labels))\n",
        "\n",
        "      #------------------------------------------VALIDATION-----------------------------------------\n",
        "      model.eval()\n",
        "      val_loss, val_correct, total_val_samples = 0.0, 0, 0\n",
        "      all_val_preds, all_val_labels, all_val_probs = [], [], []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          val_loader_tqdm = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=True)\n",
        "\n",
        "          for images, labels in val_loader_tqdm:\n",
        "              images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "              with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "                  logits = model(images)\n",
        "                  loss = loss_fn(logits, labels.long())\n",
        "\n",
        "              val_loss += loss.item() * images.size(0)\n",
        "              preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "              val_correct += ((abs(preds - labels) <= 2)).sum().item()\n",
        "              total_val_samples += labels.size(0)\n",
        "\n",
        "              all_val_preds.extend(preds.cpu().numpy())\n",
        "              all_val_labels.extend(labels.cpu().numpy())\n",
        "              all_val_probs.extend(logits.cpu().to(torch.float32).numpy())\n",
        "\n",
        "              val_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "              del logits, loss, images, labels, preds\n",
        "              xm.mark_step()\n",
        "\n",
        "      val_loss /= total_val_samples\n",
        "      val_acc = val_correct / total_val_samples\n",
        "      val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "      val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "      val_iou = jaccard_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "\n",
        "      training_history[\"val_loss\"].append(val_loss)\n",
        "      training_history[\"val_acc\"].append(val_acc)\n",
        "      training_history[\"val_precision\"].append(val_precision)\n",
        "      training_history[\"val_recall\"].append(val_recall)\n",
        "      training_history[\"val_iou\"].append(val_iou)\n",
        "      training_history[\"val_probs\"].append(np.array(all_val_probs))\n",
        "      training_history[\"val_preds\"].append(np.array(all_val_preds))\n",
        "      training_history[\"val_labels\"].append(np.array(all_val_labels))\n",
        "\n",
        "      scheduler.step(val_loss)\n",
        "      xm.rendezvous(\"lr_scheduler_sync\")\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          best_model_state = model.state_dict()\n",
        "          epochs_no_improve = 0\n",
        "\n",
        "          if xm.is_master_ordinal():\n",
        "              xm.save(best_model_state, \"best_model.pth\")\n",
        "      else:\n",
        "          epochs_no_improve += 1\n",
        "\n",
        "      if epochs_no_improve > patience:\n",
        "          print(\"Early stopping activated.\")\n",
        "          break\n",
        "\n",
        "\n",
        "      xm.rendezvous(\"cleanup\")\n",
        "    return training_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyv02Skldg7"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIwu2IwCt_ua",
        "outputId": "c62ae384-5803-4b7d-fb78-b229b105c146"
      },
      "outputs": [],
      "source": [
        "model = ModifiedResNet50Backbone()\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/training_historyBackbone10+22.pth\"))\n",
        "\n",
        "\n",
        "training_history = train_model(model, train_dataloader, val_dataloader)\n",
        "with open(\"training_history.pkl\", \"wb\") as f:\n",
        "                  pickle.dump(training_history, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nURqTUUPHAZ8",
        "i90CjgYnhIzo",
        "UbVzSmrpskSR",
        "QhwymGh-so42",
        "Rc6wdxFDDtfu",
        "le4TqXXTsuBg",
        "N2qy8Q_-xyNr",
        "BgPGez3RhM3Y",
        "_uSHtjdY9GbT"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
