{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyrWjJPcnqml"
      },
      "source": [
        "# IMPORTAR AND DOWNLOAD DEPENDECIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ludDPy29O2AR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchinfo\n",
        "!pip install albumentations\n",
        "!pip install torchmetrics\n",
        "!pip install faster-coco-eval\n",
        "!pip install torchmetrics[detection]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXa7ab50yCg5",
        "outputId": "4d5a00ee-f47b-4493-ecba-99fedf7936ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TFG'...\n",
            "remote: Enumerating objects: 470, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 470 (delta 5), reused 13 (delta 5), pack-reused 453 (from 1)\u001b[K\n",
            "Receiving objects: 100% (470/470), 126.73 KiB | 1.74 MiB/s, done.\n",
            "Resolving deltas: 100% (233/233), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf TFG\n",
        "!git clone https://github.com/Blancolote/TFG.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztQnkOO5zGe6"
      },
      "outputs": [],
      "source": [
        "from TFG.CustomFaster_MP_TPU.faster_rcnn import FasterRCNN as CustomFasterRCNN\n",
        "from TFG.CustomFaster_MP_TPU.anchor_utils import AnchorGenerator as AnchorGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKL5cFpIKW2r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.ops import box_iou\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "import random\n",
        "import os\n",
        "#os.environ[\"XLA_FLAGS\"] = \"--xla_dump_hlo_as_text --xla_dump_to=/tmp/xla\"\n",
        "#os.environ['XLA_IR_DEBUG'] = '1'\n",
        "#os.environ['XLA_HLO_DEBUG'] = '1'\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "from torchinfo import summary\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.amp.syncfree.adamw as adamw\n",
        "from torch_xla.distributed.parallel_loader import MpDeviceLoader\n",
        "import torch_xla.debug.metrics as met\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import pycocotools\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_curve, auc, confusion_matrix, f1_score\n",
        "import time\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "S9J7htXHe_Se",
        "outputId": "622db6ae-8361-4ef5-cde8-a98fcaa8fe97"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ground_truth_boxes = torch.tensor([[12,12,45,45],[12,12,45,45]], dtype=torch.float32)\\n\\n# Predicciones del modelo\\npredicted_boxes = torch.tensor([\\n    [12, 12, 48, 48],[12,12,45,45],   # buena predicci贸n de la box 1\\n], dtype=torch.float)\\n\\n# Calculamos la matriz de IoU\\nious = box_iou(predicted_boxes, ground_truth_boxes)\\n\\nprint(\"Matriz IoU:\")\\nprint(ious)\\n\\nmean_iou = ious.mean().item()\\nprint(\"IoU promedio (por predicci贸n):\", mean_iou)'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"ground_truth_boxes = torch.tensor([[12,12,45,45],[12,12,45,45]], dtype=torch.float32)\n",
        "\n",
        "# Predicciones del modelo\n",
        "predicted_boxes = torch.tensor([\n",
        "    [12, 12, 48, 48],[12,12,45,45],   # buena predicci贸n de la box 1\n",
        "], dtype=torch.float)\n",
        "\n",
        "# Calculamos la matriz de IoU\n",
        "ious = box_iou(predicted_boxes, ground_truth_boxes)\n",
        "\n",
        "print(\"Matriz IoU:\")\n",
        "print(ious)\n",
        "\n",
        "mean_iou = ious.mean().item()\n",
        "print(\"IoU promedio (por predicci贸n):\", mean_iou)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5SVAMAXySyhz",
        "outputId": "71c4d33a-621f-49f5-d92d-b90e1e9ba3e3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"ious_list = [0.7, 0.65, float('nan'), 0.8]\\n\\n# Para calcular el promedio ignorando los nan:\\nimport numpy as np\\nmean_over_dataset = np.nanmean(ious_list)\\nmean_over_dataset\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"ious_list = [0.7, 0.65, float('nan'), 0.8]\n",
        "\n",
        "# Para calcular el promedio ignorando los nan:\n",
        "import numpy as np\n",
        "mean_over_dataset = np.nanmean(ious_list)\n",
        "mean_over_dataset\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NqQ7ohcGodb"
      },
      "source": [
        "# DATALOADER\n",
        "\n",
        "This block defines the Dataloader used for the Faster R-CNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_ohw7B8JtVR"
      },
      "outputs": [],
      "source": [
        "def resizeAndTransform(bbox, original_size, new_size):\n",
        "    \"\"\"\n",
        "    Transforms a bounding box from (x, y, width, height) to (x_min, y_min, x_max, y_max)\n",
        "    and rescales it to a new size.\n",
        "    \n",
        "    Args:\n",
        "    -bbox: Bounding box in the format (x, y, width, height). x and y are the top-left corner coordinates.\n",
        "    - original_size: tuple(original_width, original_height) of the image.\n",
        "    - new_size: tuple(new_width, new_height) new size of the image.\n",
        "\n",
        "    Returns:\n",
        "    - Resized bounding box in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
        "    \"\"\"\n",
        "\n",
        "    x, y, width, height = bbox\n",
        "\n",
        "    original_width, original_height = original_size\n",
        "    new_width, new_height = new_size\n",
        "\n",
        "    x_min = x\n",
        "    y_min = y\n",
        "    x_max = x + width\n",
        "    y_max = y + height\n",
        "\n",
        "    x_min_resized = x_min * (new_width / original_width)\n",
        "    y_min_resized = y_min * (new_height / original_height)\n",
        "    x_max_resized = x_max * (new_width / original_width)\n",
        "    y_max_resized = y_max * (new_height / original_height)\n",
        "\n",
        "    x_min_resized = max(0, min(x_min_resized, new_width))\n",
        "    y_min_resized = max(0, min(y_min_resized, new_height))\n",
        "    x_max_resized = max(0, min(x_max_resized, new_width))\n",
        "    y_max_resized = max(0, min(y_max_resized, new_height))\n",
        "\n",
        "    return [x_min_resized, y_min_resized, x_max_resized, y_max_resized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l28hgx6cYaee"
      },
      "outputs": [],
      "source": [
        "def creator_path_slices(patient_dir):\n",
        "    return [os.path.join(patient_dir,s) for s in os.listdir(patient_dir)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_S2UEU-E5rZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_augmentations():\n",
        "  rotation = random.randint(-10,10)\n",
        "  return A.Compose([A.Affine(rotate=(rotation,rotation),p=0.1),\n",
        "                    A.Affine(\n",
        "                        scale=(1 - 0.15, 1 + 0.15),\n",
        "                        translate_percent=(-0.1, 0.1),\n",
        "                        p=0.8, border_mode=cv2.BORDER_CONSTANT),\n",
        "                    A.RandomBrightnessContrast(\n",
        "                    brightness_limit=0.1,\n",
        "                    contrast_limit=0.1,\n",
        "                    p=0.5\n",
        "                ),\n",
        "                A.GaussianBlur(blur_limit=(3, 5), p=0.1),\n",
        "                A.GaussNoise(std_range=(0.05, 0.1), p=0.2),\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.2, label_fields=['labels']))\n",
        "\n",
        "class TomosynthesisDataset(Dataset):\n",
        "  def __init__(self, csv_path, images_folder, augmentations=None):\n",
        "      self.metadata = pd.read_csv(csv_path)\n",
        "      self.images_folder = images_folder\n",
        "      self.augmentations = augmentations\n",
        "      self.data = []\n",
        "      self.empty_boxes = torch.zeros((0,4), dtype=torch.bfloat16)\n",
        "\n",
        "      for i in range(len(self.metadata)):\n",
        "          patient_id = self.metadata.iloc[i]['PatientID']\n",
        "          target_slice = self.metadata.iloc[i]['Slice_representativo']\n",
        "          intermediate_folder = self.metadata.iloc[i]['Intermedia']\n",
        "          patient_folder = os.path.join(images_folder, patient_id)\n",
        "          x = self.metadata.iloc[i]['X']\n",
        "          y = self.metadata.iloc[i]['Y']\n",
        "          width = self.metadata.iloc[i]['Width']\n",
        "          height = self.metadata.iloc[i]['Height']\n",
        "          classification = self.metadata.iloc[i]['Clasificacion']\n",
        "          original_width = self.metadata.iloc[i]['Original_width']\n",
        "          original_height = self.metadata.iloc[i]['Original_height']\n",
        "\n",
        "          entry = {\n",
        "              'patient_id': patient_id,\n",
        "              'target_slice': target_slice,\n",
        "              'intermediate_folder': os.path.join(patient_folder, intermediate_folder),\n",
        "              'X': x,\n",
        "              'Y': y,\n",
        "              'Width': width,\n",
        "              'Height': height,\n",
        "              'Classification': classification,\n",
        "              'Original_width': original_width,\n",
        "              'Original_height' : original_height\n",
        "          }\n",
        "\n",
        "          if classification == 1 and (random.random() < 0.6): #all images that have anomaly could be data augmented within 50% of posibilities\n",
        "            \n",
        "            self.data.append((entry, False))  # Original image\n",
        "            \n",
        "            self.data.append((entry, True))   # Data augmented image\n",
        "          else:\n",
        "            self.data.append((entry,False))\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \n",
        "      entry, apply_augmentation = self.data[idx]  \n",
        "\n",
        "      distancia = 100\n",
        "      target_slice_subsample_position = 0\n",
        "      intermediate_folder = entry['intermediate_folder']\n",
        "      target_slice = entry['target_slice']\n",
        "      x = entry['X']\n",
        "      y = entry['Y']\n",
        "      width = entry['Width']\n",
        "      heigth = entry['Height']\n",
        "      classification = entry['Classification']\n",
        "      original_width = entry['Original_width']\n",
        "      original_height = entry['Original_height']\n",
        "\n",
        "      bbox_albumentationsFormat = None\n",
        "      if classification == 0:\n",
        "        bbox_albumentationsFormat = self.empty_boxes\n",
        "      else:\n",
        "        bbox_albumentationsFormat = [resizeAndTransform((x,y,width,heigth), (original_width,original_height), (256,256))]\n",
        "\n",
        "      paths_slices = creator_path_slices(intermediate_folder)\n",
        "\n",
        "      if len(paths_slices) != 27:\n",
        "          raise ValueError(f\"Expected 27 slices, got {len(paths_slices)} for patient {entry['patient_id']}\")\n",
        "\n",
        "      for i in range(len(paths_slices)):\n",
        "          name_slice = paths_slices[i].split('/')[-1]\n",
        "          num_slice = int(name_slice.split('.')[0])\n",
        "\n",
        "          if abs(target_slice - num_slice) < distancia:\n",
        "              target_slice_subsample_position = i\n",
        "              distancia = abs(target_slice - num_slice)\n",
        "              popo = num_slice\n",
        "\n",
        "      slices = []\n",
        "      transform = transforms.ToTensor()\n",
        "\n",
        "      for path in paths_slices:\n",
        "          if not os.path.exists(path):\n",
        "              raise FileNotFoundError(f\"Slice not found: {path}\")\n",
        "\n",
        "          slice_img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "          slice_img = cv2.cvtColor(slice_img, cv2.COLOR_BGR2RGB)\n",
        "          slice_img = (slice_img / 255.0).astype(np.float32)\n",
        "\n",
        "          # Aplicar augmentaci贸n\n",
        "          if apply_augmentation==True and self.augmentations is not None:  \n",
        "              transformed = self.augmentations(image=slice_img, bboxes = np.array(bbox_albumentationsFormat, dtype=np.float32), labels=[classification] )\n",
        "              slice_img = transformed['image']\n",
        "              transformed_bbox = transformed['bboxes']\n",
        "          else:\n",
        "            transformed_bbox = bbox_albumentationsFormat\n",
        "\n",
        "\n",
        "          slice_tensor = transform(slice_img)\n",
        "          slices.append(slice_tensor)\n",
        "\n",
        "      slices_tensor = torch.stack(slices)\n",
        "\n",
        "      classification = torch.tensor([classification], dtype=torch.long)  #ensure that is not an empty tensor\n",
        "\n",
        "      if isinstance(transformed_bbox, torch.Tensor):\n",
        "        if len(transformed_bbox.shape)!=2:\n",
        "          transformed_bbox = transformed_bbox.unsqueeze(0)\n",
        "        transformed_bbox_tensor = transformed_bbox.clone().detach().float()\n",
        "      else:\n",
        "        transformed_bbox = torch.tensor(transformed_bbox, dtype=torch.bfloat16)\n",
        "        if len(transformed_bbox.shape)!=2:\n",
        "          transformed_bbox = transformed_bbox.unsqueeze(0)\n",
        "        transformed_bbox_tensor = transformed_bbox.clone().detach().float()\n",
        "\n",
        "      transformed_bbox_tensor = torch.clamp(transformed_bbox_tensor,0,256)\n",
        "\n",
        "      dic_labels = {\"boxes\": transformed_bbox_tensor, \"labels\": classification}\n",
        "      return slices_tensor, dic_labels, torch.tensor(target_slice_subsample_position, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrx1yNdFbZdz"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images, targets, positions = zip(*batch)\n",
        "    structured_targets = []\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    for t in targets:\n",
        "        if len(t) > 0:\n",
        "            # extract boxes and labels from the dictionary to ensure they are in the correct format\n",
        "            boxes = t[\"boxes\"]\n",
        "            labels = t[\"labels\"]\n",
        "\n",
        "        structured_targets.append({\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels\n",
        "        })\n",
        "\n",
        "    return images, structured_targets, torch.tensor(positions, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpeQVXKkSyE8"
      },
      "source": [
        "# INTERMEDIATE MODULE\n",
        "\n",
        "Takes the input from the bakcbone of 27 slices and turns it into a one feature map for each tomosynthesis image.\n",
        "Then a FPN is applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTEUow7YQ_BQ"
      },
      "outputs": [],
      "source": [
        "class FeatureMapMerger(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureMapMerger, self).__init__()\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[512, 1024, 2048],\n",
        "            out_channels=512     #output size of the feature map channels\n",
        "        )\n",
        "\n",
        "    def forward(self, logits, c3, c4, c5):\n",
        "\n",
        "      assert logits.ndim == 2, \"logits should have shape (batch_size, num_slices)\"\n",
        "\n",
        "      fused_features_dic = {}\n",
        "\n",
        "      list_features = [c3, c4, c5]\n",
        "\n",
        "      # Calc weights\n",
        "      weights = torch.softmax(logits, dim=1)  # (batch_size, num_slices)\n",
        "\n",
        "      positions = torch.argmax(logits, dim=1)\n",
        "\n",
        "      # From (batch_size, num_slices) to (batch_size, num_slices, 1, 1, 1)\n",
        "      expanded_weights = weights[:, :, None, None, None]\n",
        "\n",
        "      for i,feature_map in enumerate(list_features):\n",
        "\n",
        "        assert feature_map.ndim == 5, \"feature maps should have shape (batch_size, num_slices, num_feature_maps, height, width)\"\n",
        "\n",
        "        # feature_maps tiene forma (batch_size, num_slices, num_feature_maps, height, width)\n",
        "        weighted_feature_map = feature_map * expanded_weights\n",
        "\n",
        "        # One feature map per image\n",
        "        fused_features_dic[str(i)] = torch.sum(weighted_feature_map, dim=1)\n",
        "\n",
        "      fpn_out = self.fpn(\n",
        "          fused_features_dic\n",
        "      )\n",
        "\n",
        "      return fpn_out, positions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnJEYnz9S5py"
      },
      "source": [
        "# BACKBONE\n",
        "\n",
        "Backbone presented in the file backbone_training.ipynb with no FPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5HVHxg3DOf5"
      },
      "outputs": [],
      "source": [
        "class RepresentativeSliceDetector(nn.Module):\n",
        "    def __init__(self, feature_dim=512, hidden_dim=256, dropout=0.3):\n",
        "        super(RepresentativeSliceDetector, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(self.feature_dim)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "\n",
        "        features = feature_map.mean(dim=(3, 4))  \n",
        "\n",
        "        features = self.layer_norm(features)\n",
        "        logits = self.attention(features)\n",
        "\n",
        "        return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEEcL7-EZBSv"
      },
      "outputs": [],
      "source": [
        "class ResNet50FeatureExtractor(nn.Module):\n",
        "    def __init__(self, weights=ResNet50_Weights.DEFAULT, dropout=0.3):\n",
        "        super(ResNet50FeatureExtractor, self).__init__()\n",
        "        self.resnet50 = models.resnet50(weights=weights)\n",
        "        self.stem = nn.Sequential(\n",
        "            self.resnet50.conv1,\n",
        "            self.resnet50.bn1,\n",
        "            self.resnet50.relu,\n",
        "            self.resnet50.maxpool  \n",
        "        )\n",
        "\n",
        "        self.layer1 = self.resnet50.layer1  \n",
        "        self.layer2 = self.resnet50.layer2  \n",
        "        self.layer3 = self.resnet50.layer3  \n",
        "        self.layer4 = self.resnet50.layer4  \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_slices, C, H, W = x.size()\n",
        "        x = x.view(batch_size * num_slices, C, H, W)\n",
        "\n",
        "        x = self.stem(x)\n",
        "        c2 = self.layer1(x)  \n",
        "        c3 = self.layer2(c2)  \n",
        "        c4 = self.layer3(c3)  \n",
        "        c5 = self.layer4(c4)  \n",
        "\n",
        "        def reshape(f): return f.view(batch_size, num_slices, f.size(1), f.size(2), f.size(3))\n",
        "\n",
        "        return {\n",
        "            'c3': reshape(c3),\n",
        "            'c4': reshape(c4),\n",
        "            'c5': reshape(c5)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptUVXa6fwoIZ"
      },
      "outputs": [],
      "source": [
        "class ModifiedResNet50Backbone(nn.Module): \n",
        "    def __init__(self, out_channels=2048):\n",
        "        super(ModifiedResNet50Backbone, self).__init__()\n",
        "\n",
        "        try:\n",
        "          self.device = xm.xla_device()\n",
        "          self.dtype = torch.bfloat16 if 'xla' in str(self.device) else torch.float32\n",
        "        except:\n",
        "          self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "          self.dtype = torch.float16 if \"cuda\" in str(self.device) else torch.bfloat16\n",
        "\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.feature_extractor = ResNet50FeatureExtractor()\n",
        "        self.attention_module = RepresentativeSliceDetector(feature_dim=self.out_channels)\n",
        "\n",
        "        self.to(self.dtype)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        top_indices_logits = None\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        c3, c4, c5 = features[\"c3\"], features[\"c4\"], features[\"c5\"]\n",
        "\n",
        "        top_indices_logits = self.attention_module(c5)\n",
        "\n",
        "        return c3, c4, c5, top_indices_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t942Z_6q7fnx"
      },
      "outputs": [],
      "source": [
        "class CustomBackbone(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.backbone = ModifiedResNet50Backbone()\n",
        "    self.backbone.load_state_dict(torch.load(\"/content/drive/MyDrive/backbone.pth\"))\n",
        "    for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    self.intermediate = FeatureMapMerger()\n",
        "    self.out_channels = 512\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    c3 ,c4 ,c5 , logits = self.backbone(x)\n",
        "    fused_feature_map, positions= self.intermediate(logits, c3, c4, c5)\n",
        "\n",
        "    representativeSlices = x[torch.arange(x.shape[0]), positions]  #most representative slice per image\n",
        "\n",
        "\n",
        "    return fused_feature_map, logits, representativeSlices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHMc0IscTAoB"
      },
      "source": [
        "# FASTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfpunDg9C71L"
      },
      "source": [
        "## Outputs examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WsRBXA4C0_Z"
      },
      "source": [
        "In training:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "P茅rdidas: {'loss_classifier': tensor(0.6855, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(9.6547e-06, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.6846, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1400, grad_fn=<DivBackward0>)}\n",
        "Logits: tensor([[-0.4513, -0.1834,  0.0338, -0.5055, -0.5497, -0.0438,  0.2732, -0.1832,\n",
        "         -0.0653,  0.0784, -0.5180,  0.0356, -0.2929, -0.2163, -0.2548,  0.0238,\n",
        "         -0.1174, -0.3515,  0.2769, -0.2075, -0.1633, -0.0156, -0.3062, -0.1422,\n",
        "          0.1197, -0.3001, -0.2209],\n",
        "        [-0.0558, -0.0029, -0.0046, -0.0118, -0.2489, -0.1658, -0.0777,  0.1584,\n",
        "         -0.0571,  0.4947,  0.0257, -0.2142, -0.0926,  0.4675, -0.0921, -0.2695,\n",
        "          0.4509, -0.2853, -0.3968,  0.0142,  0.1585, -0.3831, -0.5032, -0.3430,\n",
        "         -0.3413, -0.3068, -0.6169],\n",
        "        [-0.2340,  0.1319, -0.0977,  0.1093, -0.4260, -0.4540,  0.0163,  0.0521,\n",
        "          0.2435,  0.3576, -0.0256, -0.3542,  0.2086,  0.0764,  0.0368, -0.0441,\n",
        "         -0.1628, -0.1285, -0.1255, -0.3494, -0.1908, -0.4324, -0.2591,  0.3431,\n",
        "         -0.3080,  0.2426, -0.1217],\n",
        "        [-0.3487, -0.0390,  0.0026,  0.0154,  0.2411, -0.1290, -0.1990, -0.7018,\n",
        "         -0.1884, -0.0971, -0.1541,  0.1258, -0.3235, -0.5454, -0.2858,  0.2599,\n",
        "         -0.0584, -0.1472,  0.2348, -0.1662, -0.0596,  0.0074, -0.3792,  0.2995,\n",
        "         -0.0809,  0.4552, -0.1480]], grad_fn=<SqueezeBackward1>)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxVaim7DDva"
      },
      "source": [
        "In inference:\n",
        "\n",
        "\n",
        "```\n",
        "Outputs: [{'boxes': tensor([[0., 0., 0., 0.]]), 'labels': tensor([0]), 'scores': tensor([1.])}, {'boxes': tensor([[0., 0., 0., 0.]]), 'labels': tensor([0]), 'scores': tensor([1.])}, {'boxes': tensor([[0., 0., 0., 0.]]), 'labels': tensor([0]), 'scores': tensor([1.])}, {'boxes': tensor([[0., 0., 0., 0.]]), 'labels': tensor([0]), 'scores': tensor([1.])}]\n",
        "Logits: tensor([[-0.1839, -0.1884, -0.1937, -0.2048, -0.1715, -0.1859, -0.1893, -0.1789,\n",
        "         -0.1866, -0.2029, -0.1892, -0.2091, -0.1994, -0.1818, -0.1888, -0.1641,\n",
        "         -0.1866, -0.1924, -0.1870, -0.1784, -0.1886, -0.1930, -0.1782, -0.1883,\n",
        "         -0.1843, -0.1954, -0.1632],\n",
        "        [-0.1678, -0.1775, -0.1724, -0.1965, -0.1880, -0.1826, -0.1909, -0.1788,\n",
        "         -0.1652, -0.1920, -0.2019, -0.2011, -0.2021, -0.1774, -0.1920, -0.1714,\n",
        "         -0.1959, -0.1871, -0.1885, -0.1809, -0.1601, -0.1790, -0.2130, -0.1581,\n",
        "         -0.1681, -0.1959, -0.1895],\n",
        "        [-0.2171, -0.1849, -0.1740, -0.1858, -0.1918, -0.2019, -0.2090, -0.1816,\n",
        "         -0.1881, -0.2034, -0.1840, -0.1888, -0.1935, -0.1629, -0.1875, -0.1790,\n",
        "         -0.2033, -0.1882, -0.1541, -0.1910, -0.2118, -0.2116, -0.1986, -0.1918,\n",
        "         -0.1800, -0.1922, -0.2059],\n",
        "        [-0.1599, -0.1789, -0.1731, -0.1946, -0.1949, -0.1954, -0.2328, -0.1558,\n",
        "         -0.1741, -0.2072, -0.1961, -0.2003, -0.1710, -0.1786, -0.1925, -0.1976,\n",
        "         -0.1944, -0.2055, -0.2001, -0.2203, -0.2082, -0.2003, -0.1960, -0.1843,\n",
        "         -0.1917, -0.1862, -0.1707]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH6aJzt_aZxS"
      },
      "source": [
        "## Custom Faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTuuutQwTCP6"
      },
      "outputs": [],
      "source": [
        "class CustomFaster(nn.Module):\n",
        "  \"\"\"\n",
        "  Modification of FasterRCNN, with a custom backbone and a intermediate module presented before.\n",
        "  Is made to be trained with a TPU and can distinguish between two classes: background and anomaly. When inference,\n",
        "  the model will select as an anomaly does outputs with more that a 0.5 score and those which boxes is\n",
        "  composed of less than 95% of dark pixels. For more information, see the source code.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.backbone = CustomBackbone()\n",
        "    try:\n",
        "      self.device = xm.xla_device()\n",
        "      self.dtype = torch.bfloat16 if 'xla' in str(self.device) else torch.float32\n",
        "\n",
        "    except:\n",
        "      if torch.cuda.is_available():\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.dtype = torch.float16\n",
        "      else:\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.dtype = torch.float32\n",
        "\n",
        "    anchor_generator = AnchorGenerator(\n",
        "      sizes = ((8, 16, 256), (32, 48, 256), (64, 96, 256)), #256 is for actionable cases\n",
        "      aspect_ratios=((0.5, 1.0, 1.5),(0.5, 1.0, 1.5),(0.5, 1.0, 1.5))\n",
        "    )\n",
        "\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "      featmap_names=['0', '1', '2'], output_size=5, sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    self.fasterRCNN = CustomFasterRCNN(\n",
        "        backbone=self.backbone,\n",
        "        num_classes=2,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "      )\n",
        "\n",
        "  def forward(self, slice_tensor, targets=None):\n",
        "    \"\"\"\n",
        "    slices_tensor: Slices tensor of size (batch_size, num_slices, C, H, W).\n",
        "    targets: Dic with 'boxes' and 'labels' (only used when training).\n",
        "    \"\"\"\n",
        "\n",
        "    slice_tensor = slice_tensor.to(self.dtype)\n",
        "    if self.training:\n",
        "      if targets is None or not isinstance(targets, list) or not all(isinstance(t, dict) for t in targets):\n",
        "        raise ValueError(f\"`Targets` should be a dic list, but {type(targets)} was passed\")\n",
        "      targets = [{k: v.to(self.dtype) if torch.is_tensor(v) and k != \"labels\"\n",
        "                           else v for k, v in t.items()} for t in targets]\n",
        "      losses, logits =  self.fasterRCNN(slice_tensor,targets )\n",
        "      return losses, logits\n",
        "\n",
        "    else:\n",
        "\n",
        "      outputs, logits = self.fasterRCNN(slice_tensor)\n",
        "\n",
        "      threshold_black_ratio = 0.90\n",
        "      pixel_threshold = 0.1\n",
        "\n",
        "      for i, output in enumerate(outputs):\n",
        "          boxes = output['boxes']\n",
        "          scores = output['scores']\n",
        "          labels = output['labels']\n",
        "\n",
        "          if (labels == 1).any():\n",
        "              anomaly_mask = (labels == 1)\n",
        "              anomaly_scores = scores[anomaly_mask]\n",
        "              anomaly_boxes = boxes[anomaly_mask]\n",
        "\n",
        "              best_idx = torch.argmax(anomaly_scores)\n",
        "              best_box = anomaly_boxes[best_idx].int()\n",
        "              x1, y1, x2, y2 = best_box.tolist()\n",
        "\n",
        "              cropped = slice_tensor[i, :, y1:y2, x1:x2]\n",
        "              black_pixel_ratio = (cropped < pixel_threshold).float().mean()\n",
        "\n",
        "              if black_pixel_ratio >= threshold_black_ratio:\n",
        "                  outputs[i] = {\n",
        "                      'boxes': torch.zeros((0, 4), device=self.device),\n",
        "                      'scores': torch.zeros((0), device=self.device),\n",
        "                      'labels': torch.zeros((0), device=self.device, dtype=torch.long)\n",
        "                  }\n",
        "\n",
        "\n",
        "\n",
        "      return outputs,logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUxGM1Ol-HlS"
      },
      "source": [
        "# Load Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwc21Y5zNgUz"
      },
      "source": [
        "## Load data from a split Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fjvDBsYUnKM"
      },
      "outputs": [],
      "source": [
        "train_dataset = TomosynthesisDataset(\n",
        "      csv_path= \"csv_train_path\",\n",
        "      images_folder='image_folder_train_path',\n",
        "      augmentations = get_augmentations()\n",
        "    )\n",
        "\n",
        "val_dataset = TomosynthesisDataset(\n",
        "  csv_path='csv_test_path',\n",
        "  images_folder='image_folder_val_path',\n",
        "  )\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        "  )\n",
        "\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iDNqWWYNkRr"
      },
      "source": [
        "## Load data from 1 big dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt-_LV3iNnXN",
        "outputId": "99b37423-9bf0-4dfe-ec86-e79c632b3d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tama帽o train batches: 768.\n",
            "Tama帽o val batches: 192.\n",
            "Distribuci贸n de clases en train_dataset:\n",
            "  - 0: 1819 im谩genes\n",
            "  - 1: 486 im谩genes\n",
            "\n",
            "Distribuci贸n de clases en val_dataset:\n",
            "  - 0: 455 im谩genes\n",
            "  - 1: 122 im谩genes\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/slices_test+valid.csv')\n",
        "labels = df['Clasificacion'].tolist()\n",
        "indices = list(range(len(df)))\n",
        "\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "half_train_size = len(train_idx) // 3\n",
        "half_val_size = len(val_idx) // 3\n",
        "\n",
        "train_idx_small = train_idx[:half_train_size]\n",
        "val_idx_small = val_idx[:half_val_size]\n",
        "\n",
        "base_dataset = TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='image_folder_path',\n",
        "    augmentations=None  \n",
        ")\n",
        "\n",
        "train_dataset = Subset(TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='/content/drive/MyDrive/test+valid_png',\n",
        "    augmentations=get_augmentations()\n",
        "), train_idx)\n",
        "\n",
        "val_dataset = Subset(TomosynthesisDataset(\n",
        "    csv_path='csv_path',\n",
        "    images_folder='image_folder_path',\n",
        "    augmentations=None\n",
        "), val_idx)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f'Tama帽o train batches: {len(train_dataset)}.')\n",
        "print(f'Tama帽o val batches: {len(val_dataset)}.')\n",
        "train_labels = [df.iloc[idx]['Clasificacion'] for idx in train_idx]\n",
        "train_class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
        "print(\"Distribuci贸n de clases en train_dataset:\")\n",
        "for class_name, count in train_class_counts.items():\n",
        "    print(f\"  - {class_name}: {count} im谩genes\")\n",
        "\n",
        "val_labels = [df.iloc[idx]['Clasificacion'] for idx in val_idx]\n",
        "val_class_counts = pd.Series(val_labels).value_counts().to_dict()\n",
        "print(\"\\nDistribuci贸n de clases en val_dataset:\")\n",
        "for class_name, count in val_class_counts.items():\n",
        "    print(f\"  - {class_name}: {count} im谩genes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf__M2xy0Fk9"
      },
      "source": [
        "## Funciones entrenamiento complementarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc7wbz7b0IYC"
      },
      "outputs": [],
      "source": [
        "def warmup_model(model, device):\n",
        "  \"\"\" Warm up the model to avoid slow first iteration and compile XLA graphs.\n",
        "  It is used one batch of 8 images, half of them with anomaly and half without it.\"\"\"\n",
        "\n",
        "  print(\"Performing XLA warm up...\")\n",
        "\n",
        "  batch_size = 8\n",
        "  dummy_images = torch.rand(batch_size, 27, 3, 256, 256, device=device)\n",
        "\n",
        "\n",
        "  dummy_labels = []\n",
        "\n",
        "  for i in range(batch_size // 4):\n",
        "      dummy_labels.append({\n",
        "          'boxes': torch.tensor([[10, 10, 50, 50]], device=device),\n",
        "          'labels': torch.tensor([1], device=device)\n",
        "      })\n",
        "\n",
        "\n",
        "  for i in range(batch_size // 4, batch_size):\n",
        "      dummy_labels.append({\n",
        "          'boxes': torch.empty(((0,4)), dtype=torch.float32),\n",
        "          'labels': torch.tensor([0], device=device)\n",
        "      })\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  with torch.no_grad():\n",
        "      with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "          output, logits = model(dummy_images, dummy_labels)\n",
        "\n",
        "          loss = sum(output.values())\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "          output, logits = model(dummy_images)\n",
        "\n",
        "  xm.mark_step()\n",
        "  print(\"Warm up complete.\")\n",
        "\n",
        "# Fucntion to reset metrics buffer \n",
        "def reset_metrics_buffer(buffer):\n",
        "  for key in buffer:\n",
        "      buffer[key] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fddaq-GYq3N",
        "outputId": "05c888d8-2535-4a10-bdec-bd889853f808"
      },
      "outputs": [],
      "source": [
        "def reduceOutputs(gt_box, outputBoxes, device = xm.xla_device):\n",
        "\n",
        "  \"\"\"Function to reduce the outputs of the model. It is used to calculate the IoU between the ground truth boxes and the output boxes.\n",
        "  There are four cases:\n",
        "  1. No ground truth boxes and no output boxes: perfect IoU for negative cases.\n",
        "  2. No ground truth boxes and output boxes: false positives.\n",
        "  3. Ground truth boxes and no output boxes: false negatives.\n",
        "  4. Ground truth boxes and output boxes: potential true positives.\"\"\"\n",
        "\n",
        "  if gt_box.size(0) == 0:\n",
        "\n",
        "    if outputBoxes.size(0) == 0:\n",
        "\n",
        "      return torch.tensor(1.0, device=device), -1 #perfect IoU for negative cases\n",
        "\n",
        "    return torch.tensor(0.0, device=device), -1 #false positives\n",
        "\n",
        "  if outputBoxes.size(0) == 0:\n",
        "    return torch.tensor(0.0, device=device), -1 #false negatives\n",
        "\n",
        "  ious = box_iou(gt_box, outputBoxes) #potential true positives\n",
        "  best_iou, position = torch.max(ious.squeeze(0), dim=0)\n",
        "\n",
        "  return best_iou, position.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ZMNOge-Kg9"
      },
      "source": [
        "## Funci贸n entrenamiento y validaci贸n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD_eTxrFBhcP"
      },
      "outputs": [],
      "source": [
        "def trainAndValid(model, train_loader, val_loader, num_epochs=1, lr=1e-4):\n",
        "    device = xm.xla_device()\n",
        "    model = model.to(device)\n",
        "\n",
        "    warmup_model(model, device)\n",
        "\n",
        "    train_loader = MpDeviceLoader(train_loader, device)\n",
        "    val_loader = MpDeviceLoader(val_loader, device)\n",
        "\n",
        "    optimizer = adamw.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=4\n",
        "    )\n",
        "\n",
        "    best_combined_score = 0\n",
        "    patience_earlyStop = 10\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "\n",
        "    dummy_box = torch.tensor([[0, 0, 1, 1]], device=device)  # dummy box to maintain consistency\n",
        "    background_label = torch.tensor([0], device=device) # dummy label for background\n",
        "\n",
        "    # Pre-alocate buffers for metrics (to avoid frequent CPU-TPU transfers)\n",
        "    train_metrics_buffer = {\n",
        "        \"loss\": 0.0,\n",
        "        \"loss_faster\": 0.0,\n",
        "        \"correct_attention\": 0,\n",
        "        \"samples\": 0\n",
        "    }\n",
        "\n",
        "    training_history = {\n",
        "        \"train_loss\": [], \"train_acc_attention\": [],\n",
        "        \"train_preds\": [], \"train_mri\": [], \"val_acc_attention\": [],\n",
        "        \"val_acc_faster_class\": [], \"val_labels_faster_class\": [],\n",
        "        \"val_preds_faster_class\": [], \"val_labels_faster_boxes\": [],\n",
        "        \"val_preds_faster_boxes\": [],  \"val_iou\": [],\n",
        "    }\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # ===== TRAINING PHASE =====\n",
        "\n",
        "        model.train()\n",
        "        reset_metrics_buffer(train_metrics_buffer)\n",
        "        train_preds = torch.tensor([], device=device)\n",
        "        train_mri = torch.tensor([], device=device)\n",
        "\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=True)\n",
        "\n",
        "        for images, labels, mri in train_loader_tqdm:\n",
        "            images = images.to(device)\n",
        "            labels_device = [{k: v.to(device) for k, v in label.items()} for label in labels]\n",
        "            mri = mri.to(device)\n",
        "\n",
        "            with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "\n",
        "                output, logits = model(images, labels_device)\n",
        "                loss = sum(output.values())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            batch_size = images.size(0)\n",
        "            with torch.no_grad():\n",
        "                train_metrics_buffer[\"loss\"] += loss.detach() * batch_size\n",
        "\n",
        "                preds_attention = torch.argmax(logits, dim=1)\n",
        "                train_metrics_buffer[\"correct_attention\"] += (abs(preds_attention - mri) <= 2).sum()\n",
        "                train_metrics_buffer[\"samples\"] += batch_size\n",
        "\n",
        "            train_loader_tqdm.set_postfix(loss=loss.detach())\n",
        "\n",
        "            train_preds = torch.cat([train_preds, preds_attention], dim=0)\n",
        "            train_mri = torch.cat([train_mri, mri], dim=0)\n",
        "\n",
        "            xm.optimizer_step(optimizer)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss = train_metrics_buffer[\"loss\"] / train_metrics_buffer[\"samples\"]\n",
        "        train_acc_attention = train_metrics_buffer[\"correct_attention\"] / train_metrics_buffer[\"samples\"]\n",
        "\n",
        "        training_history[\"train_loss\"].append(train_loss)\n",
        "        training_history[\"train_acc_attention\"].append(train_acc_attention)\n",
        "        training_history[\"train_preds\"].append(train_preds.cpu().numpy())\n",
        "        training_history[\"train_mri\"].append(train_mri.cpu().numpy())\n",
        "\n",
        "      # ===== VALIDATION PHASE =====\n",
        "        model.eval()\n",
        "\n",
        "        # Pre-alocate buffers for validation metrics\n",
        "        val_metrics = {\n",
        "            \"correct_attention\": 0,\n",
        "            \"correct_faster_class\": 0,\n",
        "            \"samples\": 0,\n",
        "            \"iou\": 0.0\n",
        "        }\n",
        "\n",
        "        val_results = {\n",
        "          \"preds_faster_class\": torch.tensor([], device=device),\n",
        "          \"labels_faster_class\": torch.tensor([], device=device),\n",
        "          \"preds_faster_boxes\": torch.tensor([], device=device),\n",
        "          \"labels_faster_boxes\": torch.tensor([], device=device),\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=True)\n",
        "\n",
        "            \n",
        "            for images, labels, mri in val_loader_tqdm:\n",
        "                images = images.to(device)\n",
        "                labels_device = [{k: v.to(device) for k, v in label.items()} for label in labels]\n",
        "                mri = mri.to(device)\n",
        "\n",
        "                with torch_xla.amp.autocast(device=device, dtype=torch.bfloat16):\n",
        "                    output, logits = model(images)\n",
        "\n",
        "                \n",
        "                preds_attention = torch.argmax(logits, dim=1)\n",
        "                val_metrics[\"correct_attention\"] += (abs(preds_attention - mri) <= 2).sum()\n",
        "                val_metrics[\"samples\"] += mri.size(0)\n",
        "\n",
        "                for i in range(len(labels)):\n",
        "\n",
        "                  iou, index = reduceOutputs(labels[i]['boxes'], output[i]['boxes'], device)\n",
        "\n",
        "                  if index == -1:\n",
        "                    # No ground truth boxes or no output boxes, or both\n",
        "                    scoreBased_index = torch.argmax(output[i]['scores'], dim=0)\n",
        "\n",
        "                    gt_box = dummy_box if labels[i]['boxes'].size(0) == 0 else labels[i]['boxes']\n",
        "                    if output[i]['boxes'].size(0) == 0:\n",
        "                      pred_box = dummy_box\n",
        "                      pred_label = background_label\n",
        "                    else:\n",
        "                      pred_box = output[i]['boxes'][scoreBased_index]\n",
        "                      pred_label = output[i]['labels'][scoreBased_index].unsqueeze(0)\n",
        "                    \n",
        "\n",
        "                    val_metrics[\"correct_faster_class\"] += (labels[i]['labels'] == pred_label)\n",
        "\n",
        "                    val_results[\"preds_faster_class\"] = torch.cat([val_results[\"preds_faster_class\"],\n",
        "                                                                  pred_label])\n",
        "                    val_results[\"labels_faster_class\"] = torch.cat([val_results[\"labels_faster_class\"],\n",
        "                                                                  labels[i]['labels']])\n",
        "\n",
        "\n",
        "                    val_results[\"labels_faster_boxes\"] = torch.cat([val_results[\"labels_faster_boxes\"], gt_box])\n",
        "                    val_results[\"preds_faster_boxes\"] = torch.cat([val_results[\"preds_faster_boxes\"], pred_box])\n",
        "\n",
        "                    val_metrics[\"iou\"] += iou  \n",
        "\n",
        "                  else: #normal case, with gt boxes and pred boxes\n",
        "                    \n",
        "                    box = output[i]['boxes'][index].unsqueeze(0)\n",
        "                    label = output[i]['labels'][index].unsqueeze(0)\n",
        "\n",
        "\n",
        "                    val_metrics[\"correct_faster_class\"] += (label == labels[i][\"labels\"]).sum().item()\n",
        "                    val_results[\"preds_faster_class\"] = torch.cat([val_results[\"preds_faster_class\"], label])\n",
        "                    val_results[\"labels_faster_class\"] = torch.cat([val_results[\"labels_faster_class\"], labels[i]['labels']])\n",
        "                    val_results[\"labels_faster_boxes\"] = torch.cat([val_results[\"labels_faster_boxes\"], labels[i]['boxes']])\n",
        "                    val_results[\"preds_faster_boxes\"] = torch.cat([val_results[\"preds_faster_boxes\"], box])\n",
        "                    val_metrics[\"iou\"] += iou\n",
        "\n",
        "            val_loader_tqdm.set_postfix()\n",
        "\n",
        "        val_acc_attention = val_metrics[\"correct_attention\"] / val_metrics[\"samples\"]\n",
        "        val_acc_faster_class = val_metrics[\"correct_faster_class\"] / val_metrics[\"samples\"]\n",
        "        val_iou = val_metrics[\"iou\"] / val_metrics[\"samples\"]\n",
        "\n",
        "\n",
        "        training_history[\"val_acc_attention\"].append(val_acc_attention)\n",
        "        training_history[\"val_acc_faster_class\"].append(val_acc_faster_class)\n",
        "        training_history[\"val_iou\"].append(val_iou)\n",
        "\n",
        "        training_history[\"val_labels_faster_class\"].append(val_results[\"labels_faster_class\"].cpu().numpy())\n",
        "        training_history[\"val_preds_faster_class\"].append(val_results[\"preds_faster_class\"].cpu().numpy())\n",
        "        training_history[\"val_labels_faster_boxes\"].append(val_results[\"labels_faster_boxes\"].cpu().numpy())\n",
        "        training_history[\"val_preds_faster_boxes\"].append(val_results[\"preds_faster_boxes\"].cpu().numpy())\n",
        "\n",
        "        combined_score = val_acc_faster_class * 0.5 + val_iou * 0.5\n",
        "        scheduler.step(combined_score)\n",
        "\n",
        "        xm.rendezvous(\"lr_scheduler_sync\")\n",
        "\n",
        "        with open(\"training_history.pkl\", \"wb\") as f:\n",
        "            pickle.dump(training_history, f)\n",
        "\n",
        "\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "        if combined_score > best_combined_score:\n",
        "            best_combined_score = combined_score\n",
        "            xm.save(best_model_state, \"best_model.pth\")\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience_earlyStop:\n",
        "            print(\"Early stopping activated.\")\n",
        "            break\n",
        "\n",
        "        # Syncronize at the end of the epoch\n",
        "        xm.mark_step()\n",
        "\n",
        "    # Final cleaning\n",
        "    xm.rendezvous(\"cleanup\")\n",
        "\n",
        "    # Print metrics \n",
        "    metrics = met.metrics_report()\n",
        "    print(metrics)\n",
        "\n",
        "    return training_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsXjkSWZ-O1l"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYdJK7lSV70t",
        "outputId": "b0d95c9e-669e-4afa-cc25-6bbe34fe363b"
      },
      "outputs": [],
      "source": [
        "model = CustomFaster()\n",
        "model.load_state_dict(torch.load(\"model_path.pth\"))\n",
        "\n",
        "training_history = trainAndValid(model, train_dataloader, val_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "1NqQ7ohcGodb",
        "LpeQVXKkSyE8",
        "EnJEYnz9S5py",
        "dfpunDg9C71L",
        "pEIBrHndO-9f",
        "wH6aJzt_aZxS",
        "YfGKKk8FBbb6",
        "dwc21Y5zNgUz",
        "4iDNqWWYNkRr",
        "tf__M2xy0Fk9",
        "L3ZMNOge-Kg9"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
